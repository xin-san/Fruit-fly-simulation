{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest\n",
    "import csv\n",
    "import pickle\n",
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "# import matplotlib.pyplot as plt\n",
    "nest.ResetKernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EINetwork:\n",
    "    def __init__(self) -> None:\n",
    "        self.unique_id = []\n",
    "        self.neurons = {}\n",
    "        self.connections = defaultdict(list)\n",
    "        self.num_neurons = 0\n",
    "        self.BasicWeight = 50\n",
    "        \n",
    "        # count of inh and exh synapse\n",
    "        self.nI = 0\n",
    "        self.nE = 0\n",
    "        \n",
    "        self.neuron_model = 'iaf_psc_alpha'\n",
    "        \n",
    "        # nest.CopyModel('stdp_synapse_hom', 'e_syn')\n",
    "        # nest.CopyModel('static_synapse', 'i_syn')\n",
    "    \n",
    "    def process_and_cache_data_csv(\n",
    "        self,\n",
    "        csv_file_path='../connectome_materials/connections_no_threshold.csv',\n",
    "        cache_file_path='../cache_file/csv_read_to_cache.pkl'\n",
    "    ):\n",
    "        if os.path.exists(cache_file_path):\n",
    "            print('\\n*** dataset already in cache!!! ***')\n",
    "            # with open(cache_file_path, 'rb') as f:\n",
    "            #     self.unique_id, self.connections = pickle.load(f)\n",
    "            #     return self.unique_id, self.connections\n",
    "            return\n",
    "            \n",
    "        print('\\n*** Processing CSV file... ***')\n",
    "        \n",
    "        data = pd.read_csv(csv_file_path)\n",
    "        data = data.drop_duplicates(subset=['pre_root_id', 'post_root_id'], keep='last')\n",
    "        self.unique_id = list(set(data['pre_root_id']).union(set(data['post_root_id'])))\n",
    "        print('\\n*** Unique_id Completed ***')\n",
    "        \n",
    "        self.connections = [[pre, post, syn, nt] for pre, post, syn, nt in zip(data['pre_root_id'], data['post_root_id'], data['syn_count'], data['nt_type'])]\n",
    "        \n",
    "        print('\\n*** Saving processed data to cache... ***')\n",
    "        with open(cache_file_path, 'wb') as f:\n",
    "            pickle.dump((self.unique_id, self.connections), f)\n",
    "        \n",
    "        # return self.unique_id, self.connections\n",
    "        return\n",
    "            \n",
    "    \n",
    "    def Create_and_Connect(self, cache_file='../cache_file/network.h5', cache_file_path='../cache_file/csv_read_to_cache.pkl'):\n",
    "        nest.ResetKernel()\n",
    "        print('\\n*** kernel has been reset ***')\n",
    "\n",
    "        if os.path.exists(cache_file):\n",
    "            print('\\n*** network has been stored previously!!! ***')\n",
    "            return\n",
    "        \n",
    "        print('\\n*** Loading dataset from cache... ***')\n",
    "        with open(cache_file_path, 'rb') as f:\n",
    "            self.unique_id, self.connections = pickle.load(f)\n",
    "\n",
    "        print('\\n*** Creating Neurons... ***')\n",
    "        all_neurons = nest.Create(self.neuron_model, n=len(self.unique_id))\n",
    "        \n",
    "        print(f'\\n*** {len(self.unique_id)} neurons have been created ***')\n",
    "        \n",
    "        print(f'\\n*** Building neurons dictionary... ***')\n",
    "        self.neurons = dict(zip(self.unique_id, all_neurons))\n",
    "        \n",
    "        print(f'\\n*** Dictionary has been created ***')\n",
    "        \n",
    "        print(f'\\n*** Creating Connections... ***')\n",
    "        \n",
    "        for [pre, post, syn, nt] in self.connections:\n",
    "            if nt == 'GABA':\n",
    "                weight = -1 * self.BasicWeight * syn\n",
    "            else:\n",
    "                weight = self.BasicWeight * syn\n",
    "            \n",
    "            nest.Connect(\n",
    "                self.neurons[pre],\n",
    "                self.neurons[post],\n",
    "                'one_to_one',\n",
    "                {\n",
    "                    'synapse_model': 'stdp_synapse',\n",
    "                    'weight': weight\n",
    "                }\n",
    "            )\n",
    "\n",
    "        print(f'\\n*** {len(self.connections)} connections have been created ***')\n",
    "\n",
    "    def store_connections(self, cache_file='../cache_file/network.h5'):\n",
    "        if os.path.exists(cache_file):\n",
    "            print('\\n*** network has been stored previously!!! ***')\n",
    "            return\n",
    "\n",
    "        # connections = nest.GetConnections()\n",
    "        \n",
    "        \n",
    "        # print('\\n*** storing the network... ***')\n",
    "        # with pd.HDFStore('../cache_file/source.h5', mode='w') as store:\n",
    "        #     source = connections.get(\n",
    "        #     ('source'), output='pandas'\n",
    "        # )\n",
    "        #     store.append('source', source, index=False)\n",
    "        #     target = connections.get(\n",
    "        #     ('target'), output='pandas'\n",
    "        # )\n",
    "        #     store.append('target', target, index=False)\n",
    "        #     weight = connections.get(\n",
    "        #     ('weight'), output='pandas'\n",
    "        # )\n",
    "        #     store.append('weight', weight, index=False)\n",
    "\n",
    "        # print('\\n*** network has been stored ***')\n",
    "        print('\\n*** storing the network... ***')\n",
    "    \n",
    "        # 使用 chunking 来分批处理连接\n",
    "        connections = nest.GetConnections()\n",
    "        total_connections = len(connections)\n",
    "        \n",
    "        chunk_size = 30000\n",
    "        with pd.HDFStore(cache_file, mode='w') as store:\n",
    "            for start in range(0, total_connections, chunk_size):\n",
    "                end = min(start + chunk_size, total_connections)\n",
    "                chunk = connections[start:end]\n",
    "                \n",
    "                # 获取当前chunk的数据\n",
    "                source = chunk.get('source', output='pandas')\n",
    "                target = chunk.get('target', output='pandas')\n",
    "                weight = chunk.get('weight', output='pandas')\n",
    "                \n",
    "                # 将数据追加到HDF5文件\n",
    "                store.append('source', source, index=False)\n",
    "                store.append('target', target, index=False)\n",
    "                store.append('weight', weight, index=False)\n",
    "            num_neurons = pd.DataFrame({'num_neurons': [len(self.unique_id)]})\n",
    "            store.append('num_neurons', num_neurons, index=False)\n",
    "        print('\\n*** network has been stored ***')\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def restore_network(self, cache_file='../cache_file/network.h5', cache_file_path='../cache_file/csv_read_to_cache.pkl'):\n",
    "\n",
    "        print('\\n*** Loading file from cache... ***')\n",
    "        with pd.HDFStore(cache_file, mode='r') as store:\n",
    "            source = store['source']\n",
    "            target = store['target']\n",
    "            weight = store['weight']\n",
    "            num_neurons = store['num_neurons']['num_neurons'].iloc[0]\n",
    "\n",
    "        # with open(cache_file_path, 'rb') as f:\n",
    "        #     self.unique_id, self.connections = pickle.load(f)\n",
    "\n",
    "        print('\\n*** Creating neurons... ***')\n",
    "        all_neurons = nest.Create(self.neuron_model, num_neurons)\n",
    "\n",
    "        print('\\n*** Reconstructing network... ***')\n",
    "        nest.Connect(\n",
    "            source.values.flatten(),\n",
    "            target.values.flatten(),\n",
    "            'one_to_one',\n",
    "            {\n",
    "                'synapse_model': 'stdp_synapse',\n",
    "                'weight': weight.values.flatten()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print('\\n*** network has been reconstructed!!! ***')\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** dataset already in cache!!! ***\n",
      "\n",
      "*** kernel has been reset ***\n",
      "\n",
      "*** Loading dataset from cache... ***\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, 'v'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m MyNETwork \u001b[38;5;241m=\u001b[39m EINetwork()\n\u001b[1;32m      2\u001b[0m MyNETwork\u001b[38;5;241m.\u001b[39mprocess_and_cache_data_csv()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mMyNETwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreate_and_Connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[113], line 57\u001b[0m, in \u001b[0;36mEINetwork.Create_and_Connect\u001b[0;34m(self, cache_file, cache_file_path)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m*** Loading dataset from cache... ***\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnections \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m*** Creating Neurons... ***\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m all_neurons \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mCreate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_model, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_id))\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, 'v'."
     ]
    }
   ],
   "source": [
    "MyNETwork = EINetwork()\n",
    "MyNETwork.process_and_cache_data_csv()\n",
    "MyNETwork.Create_and_Connect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** network has been stored previously!!! ***\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MyNETwork.store_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Loading file from cache... ***\n",
      "\n",
      "*** Creating neurons... ***\n",
      "\n",
      "*** Reconstructing network... ***\n",
      "\n",
      "*** network has been reconstructed!!! ***\n"
     ]
    }
   ],
   "source": [
    "nest.ResetKernel()\n",
    "\n",
    "MyNETwork.restore_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Loading file from cache... ***\n",
      "\n",
      "*** Creating neurons... ***\n",
      "\n",
      "*** Reconstructing network... ***\n",
      "\n",
      "*** network has been reconstructed!!! ***\n",
      "2.551696538925171 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "MyNETwork = EINetwork()\n",
    "nest.ResetKernel()\n",
    "MyNETwork.restore_network()\n",
    "end = time.time()\n",
    "print(f'{end-start} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectome_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
